{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from os.path import isfile\n",
    "from scipy.io import loadmat\n",
    "from collections import OrderedDict\n",
    "\n",
    "from config import DATASET\n",
    "from train_classifiers import train_classifier\n",
    "from utils import compute_kernel, compute_precrec\n",
    "from utils import get_labels, _n_classes, _set_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXP_NAME = 'FK'\n",
    "EXP_NAME = 'imagenet-caffe-alex'\n",
    "DIR_DATA = './feature_extraction/' + EXP_NAME + '/codes/'\n",
    "DIR_SAVE = './feature_extraction/' + EXP_NAME + '/compdata/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainList = loadmat(DIR_DATA + EXP_NAME + '_train_files.mat')\n",
    "TrainList = TrainList['train_chunks']\n",
    "TrainList = np.squeeze(TrainList)\n",
    "TrainList = np.concatenate(TrainList, axis=0)\n",
    "\n",
    "ValList = loadmat(DIR_DATA + EXP_NAME + '_val_files.mat')\n",
    "ValList = ValList['val_chunks']\n",
    "ValList = np.squeeze(ValList)\n",
    "ValList = np.concatenate(ValList, axis=0)\n",
    "\n",
    "TestList = loadmat(DIR_DATA + EXP_NAME + '_test_files.mat')\n",
    "TestList = TestList['test_chunks']\n",
    "TestList = np.squeeze(TestList)\n",
    "TestList = np.concatenate(TestList, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataList = OrderedDict()\n",
    "DataList['train'] = TrainList\n",
    "DataList['val'] = ValList\n",
    "DataList['test'] = TestList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if isfile(DIR_SAVE + 'Kernel.npy'):\n",
    "    print('Loading the kernel matrix ...')\n",
    "    K = np.load(DIR_SAVE + 'Kernel.npy')\n",
    "    print('Kernel matrix is loaded.')\n",
    "else:\n",
    "    K = compute_kernel(DataList)\n",
    "    np.save(DIR_SAVE + 'Kernel.npy', K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_vs_all(K, train_set, all_epsilon, all_kappa):\n",
    "    \n",
    "    n_classes = _n_classes()    \n",
    "    set_sizes = _set_sizes()\n",
    "\n",
    "    tr_size = 0\n",
    "    for ind, data in enumerate(DATASET):\n",
    "        if data in train_set:\n",
    "            tr_size += set_sizes[ind]\n",
    "    \n",
    "    K_tr = np.zeros((tr_size, tr_size))\n",
    "    idx = 0 \n",
    "    for ind1, tr1 in enumerate(DATASET):\n",
    "        if tr1 not in train_set:\n",
    "            continue\n",
    "        idy = 0\n",
    "        for ind2, tr2 in enumerate(DATASET):\n",
    "            if tr2 not in train_set:\n",
    "                continue\n",
    "            K_tr[idx:set_sizes[ind1]+idx,\n",
    "                 idy:set_sizes[ind2]+idy] = K[\n",
    "                sum(set_sizes[:ind1]):sum(set_sizes[:ind1+1]),\n",
    "                sum(set_sizes[:ind2]):sum(set_sizes[:ind2+1])]\n",
    "            idy = set_sizes[ind2]            \n",
    "        idx = set_sizes[ind1]\n",
    "    \n",
    "    labels_raw = get_labels(train_set)\n",
    "\n",
    "    alpha = np.array([train_classifier(K_tr, labels_raw, all_epsilon, all_kappa, nc)\n",
    "                      for nc in range(n_classes)])\n",
    "\n",
    "    return alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_score(K, alpha, train_set, test_set):\n",
    "    \n",
    "    n_classes = _n_classes()\n",
    "    set_sizes = _set_sizes()\n",
    "    \n",
    "    tr_size = 0\n",
    "    ts_size = 0\n",
    "    for ind, data in enumerate(DATASET):\n",
    "        if data in train_set:\n",
    "            tr_size += set_sizes[ind]\n",
    "        if data in test_set:\n",
    "            ts_size += set_sizes[ind]\n",
    "            \n",
    "    K_tr_ts = np.zeros((tr_size, ts_size))\n",
    "    idx = 0 \n",
    "    for ind1, tr1 in enumerate(DATASET):\n",
    "        if tr1 not in train_set:\n",
    "            continue\n",
    "        idy = 0\n",
    "        for ind2, tr2 in enumerate(DATASET):\n",
    "            if tr2 not in test_set:\n",
    "                continue\n",
    "            K_tr_ts[idx:set_sizes[ind1]+idx,\n",
    "                    idy:set_sizes[ind2]+idy] = K[\n",
    "                sum(set_sizes[:ind1]):sum(set_sizes[:ind1+1]),\n",
    "                sum(set_sizes[:ind2]):sum(set_sizes[:ind2+1])]\n",
    "            idy = set_sizes[ind2]            \n",
    "        idx = set_sizes[ind1]\n",
    "        \n",
    "    scores = np.zeros((ts_size, n_classes))\n",
    "    for ci in range(n_classes):\n",
    "        scores[:,ci] = alpha[ci,:].dot(K_tr_ts)\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = ['train']\n",
    "test_set = ['val']\n",
    "all_epsilon = np.hstack([np.arange(1, 10) * 1e-4,\n",
    "                         np.arange(1, 10) * 1e-3,\n",
    "                         np.arange(1, 11) * 1e-2])\n",
    "all_kappa = [np.inf]\n",
    "alpha_train = train_one_vs_all(K, train_set, all_epsilon, all_kappa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = ['train']\n",
    "test_set = ['val']\n",
    "all_epsilon = np.hstack([np.arange(1, 10) * 1e-4,\n",
    "                         np.arange(1, 10) * 1e-3,\n",
    "                         np.arange(1, 11) * 1e-2])\n",
    "all_kappa = [0.1, 0.2, 0.3, 0.4, 0.5, np.inf]\n",
    "if isfile(DIR_SAVE + 'alpha_train.npy'):\n",
    "    print('Loading the trained classifiers ...')\n",
    "    alpha_train = np.load(DIR_SAVE + 'alpha_train.npy')\n",
    "    print('Classifiers are loaded.')\n",
    "else:\n",
    "    alpha_train = train_one_vs_all(K, train_set, all_epsilon, all_kappa)\n",
    "    np.save(DIR_SAVE + 'alpha_train.npy', alpha_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AP = np.zeros((len(all_kappa), len(all_epsilon), _n_classes()))\n",
    "for ind_k in range(len(all_kappa)):\n",
    "    for ind_e in range(len(all_epsilon)):\n",
    "        scores = compute_score(\n",
    "            K, alpha_train[:,:,ind_k,ind_e], train_set, test_set)\n",
    "        labels = get_labels(test_set)\n",
    "        AP[ind_k,ind_e,:] = compute_precrec(scores, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mAP = np.mean(AP, axis=2)\n",
    "mAP\n",
    "k_ind, e_ind = np.where(mAP == np.max(mAP[:-1,:]))\n",
    "c_ind,  = np.where(mAP[-1,:] == np.max(mAP[-1,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_set = ['train', 'val']\n",
    "test_set = ['test']\n",
    "if isfile(DIR_SAVE + 'alpha_rob.npy'):\n",
    "    print('Loading the robust classifier ...')\n",
    "    alpha_rob = np.load(DIR_SAVE + 'alpha_rob.npy')\n",
    "    print('Classifier is loaded.')\n",
    "else:    \n",
    "    c_opt = [all_epsilon[c_ind[0]]]    \n",
    "    alpha_rob = train_one_vs_all(K, train_set, c_opt, [np.inf]).squeeze()\n",
    "    np.save(DIR_SAVE + 'alpha_rob.npy', alpha_rob)\n",
    "if isfile(DIR_SAVE + 'alpha_dro.npy'):\n",
    "    print('Loading the robust classifier ...')\n",
    "    alpha_dro = np.load(DIR_SAVE + 'alpha_dro.npy')\n",
    "    print('Classifier is loaded.')\n",
    "else:\n",
    "    epsilon_opt = [all_epsilon[e_ind[0]]]\n",
    "    kappa_opt = [all_kappa[k_ind[0]]]\n",
    "    alpha_dro = train_one_vs_all(K, train_set, epsilon_opt, kappa_opt).squeeze()\n",
    "    np.save(DIR_SAVE + 'alpha_dro.npy', alpha_dro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_dro = compute_score(K, alpha_dro, train_set, test_set) \n",
    "AP_dro = compute_precrec(scores_dro, get_labels(test_set))\n",
    "scores_rob = compute_score(K, alpha_rob, train_set, test_set) \n",
    "AP_rob = compute_precrec(scores_rob, get_labels(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AP_rob.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AP_dro.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
